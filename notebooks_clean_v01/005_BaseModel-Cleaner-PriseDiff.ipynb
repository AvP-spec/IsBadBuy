{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data-cleaning with the base models\n",
    "## Refactoring the code\n",
    "-  PrepareColsBasePipe => StandardOHETransformer (enebleing auto detection of numerical and categorical columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Base model submission total 570 plases\n",
    "- 407 -> 0.14312\n",
    "- Public Score: 0.14431  Privat Score: 0.15419 \n",
    "- 408 -> 0.14279\n",
    "\n",
    "### Base model -Cleaner submission \n",
    "- 396 -> 0.14621\n",
    "- Public Score:  0.14587  Privat Score: 0.15537\n",
    "- 397 -> 0.14534\n",
    "\n",
    "### Model Clearner + Price difference + SVC this notebook\n",
    "\n",
    "Public Score:  0.14311  Privat Score: 0.15176\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add current directory to system path for relative import of the castom pakage: avp_pckg\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## geting one level above current path \n",
    "path_absolute = os.path.dirname(os.getcwd()) \n",
    "\n",
    "if path_absolute in sys.path:\n",
    "    print(f\"alrady in sys.path: {path_absolute}\")\n",
    "else:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n",
    "    print(f\"added to sys.path: {path_absolute}\"  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report  # f1_score,  accuracy_score, recall_score, precision_score\n",
    "\n",
    "## castom packages \n",
    "from avp_pckg.avp_model_selection import plot_scores, print_scores\n",
    "from avp_pckg.avp_model_selection import cross_validate_transformer\n",
    "\n",
    "from avp_pckg.small_functions import unanimity\n",
    "from avp_pckg.TransformerCols import StandardOHETransformer\n",
    "from avp_pckg.IsBadBuy_imputePrices import Cleaner, IsBadBuy_prepare_cols\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## switch on if modifications in avp_pckg are requared for auto-reload of the pakages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data as train and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.read_csv(path_absolute + '\\\\data\\\\features_train.csv', parse_dates=['PurchDate'], index_col=0)\n",
    "features_test = pd.read_csv(path_absolute + '\\\\data\\\\features_test.csv', parse_dates=['PurchDate'], index_col=0)\n",
    "\n",
    "target_train = pd.read_csv(path_absolute + '\\\\data\\\\target_train.csv', index_col=0)\n",
    "target_test = pd.read_csv(path_absolute + '\\\\data\\\\target_test.csv', index_col=0)\n",
    "\n",
    "print(features_train.shape, target_train.shape)\n",
    "features_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set dt cols to str \n",
    "## ['VehicleAge','VehYear', 'BYRNO', 'VNST', 'VNZIP1', 'IsOnlineSale'] \n",
    "## delete cols with redundant information\n",
    "## ['PurchDate', 'VehYear', 'WheelTypeID',  'TopThreeAmericanName', 'VNST',  'Nationality',]\n",
    "preparator = IsBadBuy_prepare_cols()\n",
    "features_train = preparator.transform(features_train)\n",
    "features_test = preparator.transform(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split infromation in cols Model in SubModel (normalization)\n",
    "cleaner = Cleaner()\n",
    "features_train = cleaner.transform(features_train)\n",
    "features_test = cleaner.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputer remooved from the concideration, it makes predictions worse.\n",
    "\n",
    "# from avp_pckg.IsBadBuy_imputePrices import IsBadBuyImputer\n",
    "# imputer = IsBadBuyImputer(percent=1, verbose=0)\n",
    "# df_tmp = pd.concat([features_train, features_test])\n",
    "# imputer.fit(df_tmp)\n",
    "# features_train = imputer.transform(features_train)\n",
    "# features_test = imputer.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tree-model\n",
    "cross-validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution time= 34 s \n",
    "# from avp_pckg.avp_model_selection import cross_validate_pipe \n",
    "param_name ='max_depth'\n",
    "param_range = [3, 4, 5, 6, 7, # 8, 9, 10, #12, 14, 16, 18,\n",
    "               ]\n",
    "score_dict = cross_validate_transformer(X=features_train,\n",
    "                                y=target_train,\n",
    "                                param_name=param_name,\n",
    "                                param_range= param_range,\n",
    "                                cv=5, \n",
    "                                max_cat=25,\n",
    "                                estimator_name='tree',\n",
    "                                n_jobs=-1,\n",
    "                                )\n",
    "\n",
    "plot_scores(score_dict, param_name='max_depth tree')\n",
    "print_scores(score_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest-model with whole dataset\n",
    "cross-validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution time= 7m 30s \n",
    "param_name ='max_depth'\n",
    "param_range = [3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, \n",
    "               #16, 18, 20, 25, 30\n",
    "               ]\n",
    "score_dict = cross_validate_transformer(X=features_train,\n",
    "                                y=target_train,\n",
    "                                param_name=param_name,\n",
    "                                param_range= param_range,\n",
    "                                cv=5, \n",
    "                                max_cat=25,\n",
    "                                estimator_name='forest', # 'forest', 'logistic', 'tree',\n",
    "                                n_jobs=-1,\n",
    "                                # cols_binary = cols_binary,\n",
    "                                )\n",
    "\n",
    "plot_scores(score_dict, param_name=param_name)\n",
    "print_scores(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regresion - model with whole dataset\n",
    "cross-validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution time= 15s \n",
    "param_name ='C'\n",
    "param_range = [0.001, 0.002, 0.004, 0.01, 0.02, 0.04, 0.1, 1, 10,]\n",
    "score_dict = cross_validate_transformer(X=features_train,\n",
    "                                y=target_train,\n",
    "                                param_name=param_name,\n",
    "                                param_range= param_range,\n",
    "                                cv=5, \n",
    "                                max_cat=25,\n",
    "                                estimator_name='logistic', # 'forest', 'logistic', 'tree',\n",
    "                                n_jobs=-1,\n",
    "                                # cols_binary = cols_binary,\n",
    "                                )\n",
    "\n",
    "plot_scores(score_dict, param_name=param_name, xlog=True)\n",
    "print_scores(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine Classifier\n",
    "cross-validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution time= 43m\n",
    "param_name ='C'\n",
    "param_range = [\n",
    "   # 0.001, 0.002, 0.004, \n",
    "    0.01, 0.1, 1, \n",
    "    # 10,\n",
    "    ]\n",
    "score_dict = cross_validate_transformer(X=features_train,\n",
    "                                y=target_train,\n",
    "                                param_name=param_name,\n",
    "                                param_range= param_range,\n",
    "                                cv=5, \n",
    "                                max_cat=25,\n",
    "                                estimator_name='svc', # 'forest', 'logistic', 'tree',\n",
    "                                n_jobs=-1,\n",
    "                                # cols_binary = cols_binary,\n",
    "                                )\n",
    "\n",
    "plot_scores(score_dict, param_name=param_name, xlog=True)\n",
    "print_scores(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## too small f1 score\n",
    "# ## run time: 22 min\n",
    "# param_name = 'n_neighbors'\n",
    "# param_range = [ 3, 4, 5, 6, 7, 8, 9, 10\n",
    "#     ]\n",
    "# score_dict = cross_validate_transformer(X=features_train,\n",
    "#                                 y=target_train,\n",
    "#                                 param_name=param_name,\n",
    "#                                 param_range= param_range,\n",
    "#                                 cv=5, \n",
    "#                                 max_cat=25,\n",
    "#                                 estimator_name='kn', # 'forest', 'logistic', 'tree',\n",
    "#                                 n_jobs=-1,\n",
    "#                                 kwards_fixed = {'weights':'uniform', 'n_jobs': -1}\n",
    "#                                 )\n",
    "\n",
    "# plot_scores(score_dict, param_name=param_name, xlog=True)\n",
    "# print_scores(score_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Tree model #########################################################\n",
    "pipe_tree = Pipeline(steps=[\n",
    "('base', StandardOHETransformer(max_cat=25) ),\n",
    "('model', DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=4))\n",
    "])\n",
    "\n",
    "pipe_tree.fit(features_train, target_train['IsBadBuy'].to_numpy())\n",
    "\n",
    "pred_tree = pipe_tree.predict(features_test)\n",
    "df_pred_tree = pd.DataFrame(pred_tree, index=features_test.index)\n",
    "df_pred_tree.columns = ['tree']\n",
    "\n",
    "report = classification_report(target_test, pred_tree)\n",
    "print('tree report: \\n', report)\n",
    "print('tree pred.sum():', pred_tree.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pipe_tree.named_steps.model.feature_importances_ \n",
    "feature_names = pipe_tree.named_steps.base.get_feature_names_out()\n",
    "tree_importances = pd.Series(importances, index=feature_names)\n",
    "tree_importances.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Forest model #########################################################\n",
    "pipe_forest = Pipeline(steps=[\n",
    "('base', StandardOHETransformer(max_cat=25)),\n",
    "('model', RandomForestClassifier(class_weight='balanced', random_state=42, max_depth=7))\n",
    "])\n",
    "\n",
    "pipe_forest.fit(features_train, target_train['IsBadBuy'].to_numpy())\n",
    "pred_forest = pipe_forest.predict(features_test)\n",
    "df_pred_forest = pd.DataFrame(pred_forest, index=features_test.index)\n",
    "df_pred_forest.columns = ['forest']\n",
    "\n",
    "\n",
    "report = classification_report(target_test, pred_forest)\n",
    "print('forest report: \\n', report)\n",
    "print('forest pred.sum():', pred_forest.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pipe_forest.named_steps.model.feature_importances_ \n",
    "feature_names = pipe_tree.named_steps.base.get_feature_names_out()\n",
    "tree_importances = pd.Series(importances, index=feature_names)\n",
    "tree_importances.sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic #########################################################\n",
    "pipe_reg = Pipeline(steps=[\n",
    "('preprocessing', StandardOHETransformer(max_cat=25)),\n",
    "('model', LogisticRegression(class_weight='balanced', random_state=42, C=0.02))\n",
    "])\n",
    "\n",
    "pipe_reg.fit(features_train, target_train['IsBadBuy'].to_numpy())\n",
    "pred_reg = pipe_reg.predict(features_test)\n",
    "df_pred_reg = pd.DataFrame(pred_reg, index=features_test.index)\n",
    "df_pred_reg.columns = ['logistic']\n",
    "\n",
    "report = classification_report(target_test, pred_reg)\n",
    "print('regression report: \\n', report)\n",
    "print('logistig pred.sum():', pred_reg.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run time 12min\n",
    "### SVC #########################################################\n",
    "pipe_svc = Pipeline(steps=[\n",
    "('preprocessing', StandardOHETransformer(max_cat=25)),\n",
    "('model', SVC(class_weight='balanced', random_state=42, C=0.1))\n",
    "])\n",
    "\n",
    "pipe_svc.fit(features_train, target_train['IsBadBuy'].to_numpy())\n",
    "pred_svc = pipe_svc.predict(features_test)\n",
    "df_pred_svc = pd.DataFrame(pred_svc, index=features_test.index)\n",
    "df_pred_svc.columns = ['svc']\n",
    "\n",
    "report = classification_report(target_test, pred_svc)\n",
    "print('regression report: \\n', report)\n",
    "print('pred.sum():', pred_svc.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# ### KNeighborsClassifier #########################################################\n",
    "# pipe_kn = Pipeline(steps=[\n",
    "# ('preprocessing', StandardOHETransformer(max_cat=25)),\n",
    "# ('model', KNeighborsClassifier(n_neighbors=3, p=2, weights='uniform', leaf_size=100, n_jobs=-1))\n",
    "# ])\n",
    "\n",
    "# pipe_kn.fit(features_train, target_train['IsBadBuy'].to_numpy())\n",
    "# pred_kn = pipe_kn.predict(features_test)\n",
    "# df_pred_kn = pd.DataFrame(pred_kn, index=features_test.index)\n",
    "# df_pred_kn.columns = ['kn']\n",
    "\n",
    "# report = classification_report(target_test, pred_kn)\n",
    "# print('regression report: \\n', report)\n",
    "# print('logistig pred.sum():', pred_svc.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check function for unanimity vote\n",
    "from avp_pckg.small_functions import unanimity\n",
    "pred_lst = [df_pred_tree, df_pred_forest, df_pred_reg,  df_pred_svc, ] # df_pred_kn\n",
    "result = unanimity(pred_lst)\n",
    "\n",
    "report = classification_report(target_test, result)\n",
    "print('result report: \\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare workflow for prediction of aim data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original train data contains target column 'IsBadBuy', therefore\n",
    "- First step is constract similar structure from features_train (origenal train data contained target column)\n",
    "- Second step is to build prediction workflow \n",
    "- Third step is to check with feature_test the total workflow\n",
    "- Forth step is to make final prediction on aim data (test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 1. constract similar structure from features_train\n",
    "features_train = pd.read_csv(path_absolute + '\\\\data\\\\features_train.csv', parse_dates=['PurchDate'], index_col=0)\n",
    "features_test = pd.read_csv(path_absolute + '\\\\data\\\\features_test.csv', parse_dates=['PurchDate'], index_col=0)\n",
    "\n",
    "target_train = pd.read_csv(path_absolute + '\\\\data\\\\target_train.csv', index_col=0)\n",
    "target_test = pd.read_csv(path_absolute + '\\\\data\\\\target_test.csv', index_col=0)\n",
    "\n",
    "train =  features_train.join(target_train, \n",
    "                             sort='index'\n",
    "                             )\n",
    "test = features_test\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 2. constract prediction workflow \n",
    "\n",
    "col_target = 'IsBadBuy'\n",
    "\n",
    "def basis_model_predict(train:pd.DataFrame, test, col_target:str ):\n",
    "    \n",
    "    ## preprocessing \n",
    "    target = train['IsBadBuy'].to_numpy()\n",
    "    features_train = train.drop(columns=[col_target])\n",
    "    features_test = test\n",
    "    \n",
    "    preparator = IsBadBuy_prepare_cols()\n",
    "    features_train = preparator.transform(features_train)\n",
    "    features_test = preparator.transform(features_test)\n",
    "    cleaner = Cleaner()\n",
    "    features_train = cleaner.transform(features_train)\n",
    "    features_test = cleaner.transform(features_test)\n",
    "    \n",
    "    prepare_cols = StandardOHETransformer(max_cat=25)  \n",
    "    mtrx_train = prepare_cols.fit_transform(features_train)\n",
    "    mtrx_test = prepare_cols.transform(features_test)\n",
    "\n",
    "    ## models\n",
    "    model_tree =  DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=4)\n",
    "    model_forest = RandomForestClassifier(class_weight='balanced', random_state=42, max_depth=7)\n",
    "    model_logreg = LogisticRegression(class_weight='balanced', random_state=42, C=0.02)\n",
    "   # model_svc = SVC(class_weight='balanced', random_state=42, C=0.1)\n",
    "\n",
    "    model_tree.fit(mtrx_train, target)\n",
    "    print('model tree fit finished')\n",
    "    model_forest.fit(mtrx_train, target)\n",
    "    print('model forest fit finished')\n",
    "    model_logreg.fit(mtrx_train, target)\n",
    "    print('model logistic regression fit finished')\n",
    "   # model_svc.fit(mtrx_train, target)\n",
    "   # print('model SVC fit finished')\n",
    "\n",
    "    pred_tree = pd.DataFrame(model_tree.predict(mtrx_test), index=features_test.index)\n",
    "    pred_forest = pd.DataFrame(model_forest.predict(mtrx_test), index=features_test.index)\n",
    "    pred_logreg = pd.DataFrame(model_logreg.predict(mtrx_test), index=features_test.index)\n",
    "   # pred_svc = pd.DataFrame(model_svc.predict(mtrx_test), index=features_test.index)\n",
    "\n",
    "    result = unanimity([pred_tree, pred_forest, pred_logreg,# pred_svc\n",
    "                        ]) # \n",
    "    df_result = pd.DataFrame(result)\n",
    "    # print(df_result.columns)\n",
    "    df_result = df_result.rename(columns={'sum': col_target })\n",
    "    return df_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = basis_model_predict(train, test, col_target,)\n",
    "# result.head()\n",
    "\n",
    "## step 3. Check with feature_test the total workflow\n",
    "report = classification_report(target_test, result) # .sort_index()\n",
    "print('result report: \\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 4. The final prediction on aim data\n",
    "\n",
    "data_train = pd.read_csv(path_absolute + '\\\\data\\\\DontGetKicked\\\\training.csv', \n",
    "                         parse_dates=['PurchDate'], \n",
    "                         index_col=0)\n",
    "# data_train.loc[:, 'WheelTypeID'] = data_train['WheelTypeID'].astype(str)\n",
    "# data_train.head()\n",
    "\n",
    "data_test = pd.read_csv(path_absolute + '\\\\data\\\\DontGetKicked\\\\test.csv', \n",
    "                         parse_dates=['PurchDate'], \n",
    "                         index_col=0)\n",
    "# data_test.loc[:, 'WheelTypeID'] = data_test['WheelTypeID'].astype(str)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_aim = basis_model_predict(data_train, data_test, col_target)\n",
    "pred_aim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_aim.to_csv(path_absolute + '\\\\data\\\\Model_predictions\\\\model_clean.csv') # price-diff _SVC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
