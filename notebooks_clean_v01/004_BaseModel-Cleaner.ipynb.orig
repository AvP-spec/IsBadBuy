{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data-cleaning with the base models\n",
    "- decision tree \n",
    "- random forest\n",
    "- logistic regression\n",
    "\n",
    "### Base model submission total 570 plases\n",
    "- 407 -> 0.14312\n",
    "- Public Score: 0.14431  Privat Score: 0.15419 \n",
    "- 408 -> 0.14279\n",
    "\n",
    "### Base model -Cleaner submission (this notebook)\n",
    "- 396 -> 0.14621\n",
    "- Public Score:  0.14587  Privat Score: 0.15537\n",
    "- 397 -> 0.14534"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add current directory to system path for relative import of the castom pakage: avp_pckg\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## geting one level above current path \n",
    "path_absolute = os.path.dirname(os.getcwd()) \n",
    "\n",
    "if path_absolute in sys.path:\n",
    "    print(f\"alrady in sys.path: {path_absolute}\")\n",
    "else:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n",
    "    print(f\"added to sys.path: {path_absolute}\"  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report  # f1_score,  accuracy_score, recall_score, precision_score\n",
    "\n",
    "## castom package \n",
    "from avp_pckg.DataFrame import AvPdataFrame \n",
    "from avp_pckg.avp_model_selection import cross_validate_pipe \n",
    "from avp_pckg.avp_model_selection import plot_scores, print_scores\n",
    "from avp_pckg.avp_model_selection import PrepareColsBase # PrepareColsTEncoder\n",
    "from avp_pckg.IsBadBuy_functions import clean_df\n",
    "from avp_pckg.small_functions import unanimity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## switch on if modifications in avp_pckg are requared for auto-reload of the pakages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data as train and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types of the coulumns needs to be checked. In current dataset 'WheelTypeID' column have a mixed datatype and should be handled separatrly \n",
    "\n",
    "not solved: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.read_csv(path_absolute + '\\\\data\\\\features_train.csv', parse_dates=['PurchDate'], index_col=0)\n",
    "features_test = pd.read_csv(path_absolute + '\\\\data\\\\features_test.csv', parse_dates=['PurchDate'], index_col=0)\n",
    "\n",
    "target_train = pd.read_csv(path_absolute + '\\\\data\\\\target_train.csv', index_col=0)\n",
    "target_test = pd.read_csv(path_absolute + '\\\\data\\\\target_test.csv', index_col=0)\n",
    "\n",
    "# # features_train.loc[:, 'WheelTypeID'] = features_train['WheelTypeID'].astype(str)\n",
    "# features_train.loc[:, 'WheelTypeID'] = features_train['WheelTypeID'].astype(float)\n",
    "# features_train.loc[:, 'WheelTypeID'] = features_train['WheelTypeID'].astype(str)\n",
    "# print(features_train.shape, target_train.shape)\n",
    "# features_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaner.clean_model(features_train)\n",
    "features_train = clean_df(features_train)\n",
    "features_test = clean_df(features_test)\n",
    "\n",
    "features_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_train.shape)\n",
    "features_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Select categorical and numerical columns\n",
    "For the base model all columns are used (exept 'PurchDate': droped in pipeline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat = ['Auction', 'VehicleAge', 'Make',  'WheelType', 'BYRNO', 'VNZIP1', # cols to use\n",
    "            # 'Trim', # too complex\n",
    "            # 'Model', 'SubModel', ## to clean\n",
    "            'ModelShort', 'WD', 'model_L', 'Type',  \n",
    "             'IsOnlineSale', 'Transmission', 'Color',  'PRIMEUNIT', 'AUCGUART', 'Size', \n",
    "           # 'VehYear', 'WheelTypeID', 'TopThreeAmericanName', ## redundency\n",
    "           # 'VNST', 'Nationality',\n",
    "            ] # cols to drop\n",
    "\n",
    "cols_num = ['MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\n",
    "            'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice',\n",
    "            'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n",
    "            'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice', \n",
    "            'VehOdo', 'VehBCost', 'WarrantyCost']\n",
    "\n",
    "cols_binary = [ 'EXT', 'GRAND', 'PICKUP', 'Multiple', 'SOLARA', 'Unspecified', 'SPORT',\n",
    "       'HighOutput', 'V6', 'V8', '4C', '6C', '5C', 'V', 'I4', 'I5', 'I6', '2V',\n",
    "       '4V', '2B', '4B', '1500', '2500', 'EFI', 'MFI', 'MPI', 'SFI', 'DOHC',\n",
    "       'SOHC', 'DI', 'SMPI', 'SPI', 'XL', '24V', '16V', 'NatAsp', 'MR2'   \n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tree-model\n",
    "cross-validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution time= 34 s \n",
    "\n",
    "# from avp_pckg.avp_model_selection import cross_validate_pipe \n",
    "param_name ='max_depth'\n",
    "param_range = [3, 4, 5, 6, 7, # 8, 9, 10, 12, 14, 16, 18,\n",
    "               ]\n",
    "score_dict = cross_validate_pipe(X=features_train,\n",
    "                                y=target_train,\n",
    "                                cols_cat=cols_cat,\n",
    "                                cols_num=cols_num,\n",
    "                                param_name=param_name,\n",
    "                                param_range= param_range,\n",
    "                                cv=5, \n",
    "                                max_cat=25,\n",
    "                                estimator_name='tree',\n",
    "                                n_jobs=-1,\n",
    "                                pipe_name = 'base',\n",
    "                                cols_binary = cols_binary,\n",
    "                                )\n",
    "\n",
    "plot_scores(score_dict, param_name='max_depth tree')\n",
    "print_scores(score_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest-model with whole dataset\n",
    "cross-validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution time= 4m 30s \n",
    "param_name ='max_depth'\n",
    "param_range = [3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 18, 20, 25, 30\n",
    "               ]\n",
    "score_dict = cross_validate_pipe(X=features_train,\n",
    "                                y=target_train,\n",
    "                                cols_cat=cols_cat,\n",
    "                                cols_num=cols_num,\n",
    "                                param_name=param_name,\n",
    "                                param_range= param_range,\n",
    "                                cv=5, \n",
    "                                max_cat=25,\n",
    "                                estimator_name='forest', # 'forest', 'logistic', 'tree',\n",
    "                                n_jobs=-1,\n",
    "                                # cols_binary = cols_binary,\n",
    "                                )\n",
    "\n",
    "plot_scores(score_dict, param_name=param_name)\n",
    "print_scores(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regresion - model with whole dataset\n",
    "cross-validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution time= 15s \n",
    "param_name ='C'\n",
    "param_range = [0.001, 0.002, 0.004, 0.01, 0.02, 0.04, 0.1, 1, 10,]\n",
    "score_dict = cross_validate_pipe(X=features_train,\n",
    "                                y=target_train,\n",
    "                                cols_cat=cols_cat,\n",
    "                                cols_num=cols_num,\n",
    "                                param_name=param_name,\n",
    "                                param_range= param_range,\n",
    "                                cv=5, \n",
    "                                max_cat=25,\n",
    "                                estimator_name='logistic', # 'forest', 'logistic', 'tree',\n",
    "                                n_jobs=-1,\n",
    "                                # cols_binary = cols_binary,\n",
    "                                )\n",
    "\n",
    "plot_scores(score_dict, param_name=param_name, xlog=True)\n",
    "print_scores(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation\n",
    "| model | parameter | f1_cv | precision  | recall | f1-score | support |\n",
    "|---|---|---|---|---|---|---|\n",
    "| Tree | depth=4 | 0.376 |- |- | - | - |\n",
    "| Forest | depth=15 | 0.411 | - | - |- | - |\n",
    "| LogReg | C=0.04 | 0.378  | - | - |- | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Tree model #########################################################\n",
    "pipe_tree = Pipeline(steps=[\n",
    "('base', PrepareColsBase(cols_cat=cols_cat, \n",
    "                         cols_num=cols_num, \n",
    "                         cols_binary = cols_binary,\n",
    "                         max_cat=25).make_pipe()),\n",
    "('model', DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=4))\n",
    "])\n",
    "\n",
    "pipe_tree.fit(features_train, target_train['IsBadBuy'].to_numpy())\n",
    "\n",
    "pred_tree = pipe_tree.predict(features_test)\n",
    "df_pred_tree = pd.DataFrame(pred_tree, index=features_test.index)\n",
    "df_pred_tree.columns = ['tree']\n",
    "\n",
    "report = classification_report(target_test, pred_tree)\n",
    "print('tree report: \\n', report)\n",
    "print('tree pred.sum():', pred_tree.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pipe_tree.named_steps.model.feature_importances_ \n",
    "feature_names = pipe_tree[:-1].get_feature_names_out()\n",
    "tree_importances = pd.Series(importances, index=feature_names)\n",
    "tree_importances.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Forest model #########################################################\n",
    "pipe_forest = Pipeline(steps=[\n",
    "('base', PrepareColsBase(cols_cat=cols_cat, \n",
    "                         cols_num=cols_num, \n",
    "                         cols_binary = cols_binary,\n",
    "                         max_cat=25).make_pipe()),\n",
    "('model', RandomForestClassifier(class_weight='balanced', random_state=42, max_depth=7))\n",
    "])\n",
    "\n",
    "pipe_forest.fit(features_train, target_train['IsBadBuy'].to_numpy())\n",
    "pred_forest = pipe_forest.predict(features_test)\n",
    "df_pred_forest = pd.DataFrame(pred_forest, index=features_test.index)\n",
    "df_pred_forest.columns = ['forest']\n",
    "\n",
    "\n",
    "report = classification_report(target_test, pred_forest)\n",
    "print('forest report: \\n', report)\n",
    "print('forest pred.sum():', pred_forest.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pipe_forest.named_steps.model.feature_importances_ \n",
    "feature_names = pipe_tree[:-1].get_feature_names_out()\n",
    "tree_importances = pd.Series(importances, index=feature_names)\n",
    "tree_importances.sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic #########################################################\n",
    "pipe_reg = Pipeline(steps=[\n",
    "('preprocessing', PrepareColsBase(cols_cat=cols_cat, \n",
    "                                  cols_num=cols_num,\n",
    "                                  cols_binary = cols_binary, \n",
    "                                  max_cat=25).make_pipe()),\n",
    "('model', LogisticRegression(class_weight='balanced', random_state=42, C=0.02))\n",
    "])\n",
    "\n",
    "pipe_reg.fit(features_train, target_train['IsBadBuy'].to_numpy())\n",
    "pred_reg = pipe_reg.predict(features_test)\n",
    "df_pred_reg = pd.DataFrame(pred_reg, index=features_test.index)\n",
    "df_pred_reg.columns = ['logistic']\n",
    "\n",
    "report = classification_report(target_test, pred_reg)\n",
    "print('regression report: \\n', report)\n",
    "print('logistig pred.sum():', pred_reg.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "Cross-Validation\n",
    "| model | parameter | f1_cv | precision  | recall | f1-score | pred.sum() |\n",
    "|---|---|---|---|---|---|---|\n",
    "| Tree | depth=4 | 0.376 |0.29 |0.49  |0.37 | 2163 |\n",
    "| Forest | depth=15 |0.411 | 0.35 |  0.49  |0.41  | 1836 |\n",
    "| LogReg | C=0.04 | 0.378 | 0.26  | 0.64  |0.37 | 3173 |\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 603fd9779956a8b3baeda3ac3fe01b3152a14b93
    "## Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.concat([df_pred_tree, df_pred_forest, df_pred_reg], axis=1)\n",
    "df_pred = pd.concat([df_pred_tree, df_pred_forest, df_pred_reg], axis=1)\n",
    "df_pred.loc[:, 'sum'] = df_pred['tree'] + df_pred['forest'] + df_pred['logistic']\n",
    "df_pred.loc[:, 'result1'] = round((df_pred['sum']+1)/3).astype(int)\n",
    "df_pred.loc[:, 'result2'] = round((df_pred['sum']+0)/3).astype(int)\n",
    "df_pred.loc[:, 'result3'] = round((df_pred['sum']-1)/3).astype(int)\n",
    "\n",
    "# display(df_pred.head())\n",
    "# print(df_pred.sum())\n",
    "\n",
    "report = classification_report(target_test, df_pred['result3'])\n",
    "print('result3 report: \\n', report)\n",
    "\n",
    "report = classification_report(target_test, df_pred['result2'])\n",
    "print('result2: \\n', report)\n",
    "\n",
    "report = classification_report(target_test, df_pred['result1'])\n",
    "print('result1: \\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "Cross-Validation\n",
    "| model | parameter | f1_cv | precision  | recall | f1-score | pred.sum() |\n",
    "|---|---|---|---|---|---|---|\n",
    "| Tree | depth=4 | 0.376 |0.30 |0.49  |0.37 | 2162 |\n",
    "| Forest | depth=10 |0.382 | 0.24 |  0.60 |0.35 | 3143 |\n",
    "| LogReg | C=0.01 | 0.375 | 0.19  | 0.80  |0.30 | 5548 |\n",
    "|Ensamble prec| - |- |0.32|0.48|0.38|1909|\n",
    "|Ensamble recall| - |- |0.18|0.81|0.30|5752|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 603fd9779956a8b3baeda3ac3fe01b3152a14b93
    "### Conclusion: \n",
    "the best results in f1 score were achieved by unanimous vote of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check function for unanimity vote\n",
    "from avp_pckg.small_functions import unanimity\n",
    "pred_lst = [df_pred_tree, df_pred_forest, df_pred_reg] # df_pred_forest,\n",
    "result = unanimity(pred_lst)\n",
    "\n",
    "report = classification_report(target_test, result)\n",
    "print('result report: \\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare workflow for prediction of aim data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original train data contains target column 'IsBadBuy', therefore\n",
    "- First step is constract similar structure from features_train (origenal train data contained target column)\n",
    "- Second step is to build prediction workflow \n",
    "- Third step is to check with feature_test the total workflow\n",
    "- Forth step is to make final prediction on aim data (test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 1. constract similar structure from features_train\n",
    "features_train = pd.read_csv(path_absolute + '\\\\data\\\\features_train.csv', parse_dates=['PurchDate'], index_col=0)\n",
    "features_test = pd.read_csv(path_absolute + '\\\\data\\\\features_test.csv', parse_dates=['PurchDate'], index_col=0)\n",
    "\n",
    "target_train = pd.read_csv(path_absolute + '\\\\data\\\\target_train.csv', index_col=0)\n",
    "target_test = pd.read_csv(path_absolute + '\\\\data\\\\target_test.csv', index_col=0)\n",
    "\n",
    "train =  features_train.join(target_train, \n",
    "                             sort='index'\n",
    "                             )\n",
    "test = features_test\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 2. constract prediction workflow \n",
    "\n",
    "cols_cat = ['Auction', 'VehicleAge', 'Make',  'WheelType', 'BYRNO', 'VNZIP1', # cols to use\n",
    "            # 'Trim', # too complex\n",
    "            # 'Model', 'SubModel', ## to clean\n",
    "            'ModelShort', 'WD', 'model_L', 'Type',  \n",
    "             'IsOnlineSale', 'Transmission', 'Color',  'PRIMEUNIT', 'AUCGUART', 'Size', \n",
    "           # 'VehYear', 'WheelTypeID', 'TopThreeAmericanName', ## redundency\n",
    "           # 'VNST', 'Nationality',\n",
    "            ] # cols to drop\n",
    "\n",
    "cols_num = ['MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\n",
    "            'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice',\n",
    "            'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n",
    "            'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice', \n",
    "            'VehOdo', 'VehBCost', 'WarrantyCost']\n",
    "\n",
    "cols_binary = [ 'EXT', 'GRAND', 'PICKUP', 'Multiple', 'SOLARA', 'Unspecified', 'SPORT',\n",
    "       'HighOutput', 'V6', 'V8', '4C', '6C', '5C', 'V', 'I4', 'I5', 'I6', '2V',\n",
    "       '4V', '2B', '4B', '1500', '2500', 'EFI', 'MFI', 'MPI', 'SFI', 'DOHC',\n",
    "       'SOHC', 'DI', 'SMPI', 'SPI', 'XL', '24V', '16V', 'NatAsp', 'MR2'   \n",
    "              ]\n",
    "\n",
    "\n",
    "col_target = 'IsBadBuy'\n",
    "\n",
    "def basis_model_predict(train, \n",
    "                        test, \n",
    "                        cols_cat, \n",
    "                        cols_num, \n",
    "                        col_target,\n",
    "                        cols_binary,\n",
    "                        ):\n",
    "    \n",
    "    target = train['IsBadBuy'].to_numpy()\n",
    "    features_train = train.drop(columns=[col_target])\n",
    "    features_train = clean_df(features_train)\n",
    "    features_test = clean_df(test.sort_index())\n",
    "    #features_test = test)\n",
    "    \n",
    "    print(f'{features_train.shape=}' + ' should be (62035, 110)') # \n",
    "    print(f'{features_test.shape=}' + ' should be (10948, 110)') \n",
    "\n",
    "    ## preprocessing \n",
    "    prepare_pipe = PrepareColsBase(cols_cat=cols_cat, \n",
    "                                   cols_num=cols_num, \n",
    "                                   cols_binary=cols_binary,\n",
    "                                   max_cat=25).make_pipe()\n",
    "    prepare_pipe.fit(features_train, target)\n",
    "    data_pipe_train = prepare_pipe.transform(features_train)\n",
    "    data_pipe_test = prepare_pipe.transform(features_test)\n",
    "\n",
    "    ## models\n",
    "    model_tree =  DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=4)\n",
    "    model_forest = RandomForestClassifier(class_weight='balanced', random_state=42, max_depth=7)\n",
    "    model_logreg = LogisticRegression(class_weight='balanced', random_state=42, C=0.02)\n",
    "\n",
    "\n",
    "    model_tree.fit(data_pipe_train, target)\n",
    "    print('model tree fit')\n",
    "    model_forest.fit(data_pipe_train, target)\n",
    "    print('model forest fit')\n",
    "    model_logreg.fit(data_pipe_train, target)\n",
    "    print('model logistic regression fit')\n",
    "\n",
    "    pred_tree = pd.DataFrame(model_tree.predict(data_pipe_test), index=features_test.index)\n",
    "    pred_forest = pd.DataFrame(model_forest.predict(data_pipe_test), index=features_test.index)\n",
    "    pred_logreg = pd.DataFrame(model_logreg.predict(data_pipe_test), index=features_test.index)\n",
    "\n",
    "\n",
    "    result = unanimity([pred_tree, pred_forest, pred_logreg]) # \n",
    "    df_result = pd.DataFrame(result)\n",
    "    # print(df_result.columns)\n",
    "    df_result = df_result.rename(columns={'sum': col_target })\n",
    "    return df_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = basis_model_predict(train, test, cols_cat, cols_num, col_target, cols_binary)\n",
    "# result.head()\n",
    "\n",
    "## step 3. Check with feature_test the total workflow\n",
    "report = classification_report(target_test.sort_index(), result)\n",
    "print('result report: \\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 4. The final prediction on aim data\n",
    "\n",
    "data_train = pd.read_csv(path_absolute + '\\\\data\\\\DontGetKicked\\\\training.csv', \n",
    "                         parse_dates=['PurchDate'], \n",
    "                         index_col=0)\n",
    "data_train.loc[:, 'WheelTypeID'] = data_train['WheelTypeID'].astype(str)\n",
    "data_train.head()\n",
    "\n",
    "data_test = pd.read_csv(path_absolute + '\\\\data\\\\DontGetKicked\\\\test.csv', \n",
    "                         parse_dates=['PurchDate'], \n",
    "                         index_col=0)\n",
    "data_test.loc[:, 'WheelTypeID'] = data_test['WheelTypeID'].astype(str)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_aim = basis_model_predict(data_train, data_test, cols_cat, cols_num, col_target, cols_binary)\n",
    "pred_aim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_aim.to_csv(path_absolute + '\\\\data\\\\01_Basis_Model_prediction\\\\basis_model_clean01.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
